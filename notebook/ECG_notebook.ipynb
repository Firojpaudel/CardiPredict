{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zkG-XkMaTGDw"
   },
   "source": [
    "---\n",
    "# Signal Processing \n",
    "#### 1. Getting MIT Data Base  https://physionet.org/physiobank/database/html/mitdbdir/intro.htm\n",
    "#### 2. Selecting Relevant Classes \n",
    "#### 3. HB Isolation Algorithm\n",
    "#### 4. Filtering and Resampling\n",
    "#### 5. Training The Data\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R7QVf79KTGDw"
   },
   "source": [
    "\n",
    "### Importing Relevant Packages \n",
    ">Make sure heartbeat.py and normalizer.py are in the same directory !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mLaQ8TZzTGDx",
    "outputId": "8b8c477e-6a7f-40cd-e642-978b519b4668"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import heartbeat as hb\n",
    "import normalizer\n",
    "import importlib\n",
    "import random \n",
    "import time \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from scipy import signal\n",
    "from scipy.signal import find_peaks, resample\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import os \n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import sys\n",
    "import warnings \n",
    "importlib.reload(hb)\n",
    "importlib.reload(normalizer)\n",
    "\n",
    "print('Packages Loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "af2ugOYpTGD2"
   },
   "source": [
    "### Data is in the directory in a folder call mit_data with csv and txt files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N78Mf_U7TGD2",
    "outputId": "f062257b-b161-45f9-95ab-94919fec9b5f"
   },
   "outputs": [],
   "source": [
    "print('Files in Directory {}:\\n'.format(os.getcwd()+'/mit_data'))\n",
    "print('-------------------------------\\n')\n",
    "onlyfiles = [f for f in listdir(os.getcwd()+'/mit_data') if isfile(join(os.getcwd()+'/mit_data', f))]\n",
    "print(sorted(onlyfiles)[1:],'\\n\\n***',len(onlyfiles)-1,'data files ***')   \n",
    "\n",
    "print('\\nAll patients:\\n',hb.all_patients())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MPi5f3RHTGD5"
   },
   "source": [
    "### All Types of Heat Beat Conditions from the Data Base\n",
    "https://physionet.org/physiobank/annotations.shtml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fc5g35AYTGD5",
    "outputId": "7697a750-0194-4946-986f-4aba9a5194bd"
   },
   "outputs": [],
   "source": [
    "print(\"MIT DATABASES CONDITIONS:\\n\")\n",
    "hb.classes_further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IUBafXtcTGD8",
    "outputId": "fdb9b436-a33e-4a11-e6b8-a38c66fd8781"
   },
   "outputs": [],
   "source": [
    "hb.most_common_conditions(patients=hb.all_patients(),top_k=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "086jHwBtTGD_"
   },
   "source": [
    "### Classes we will be examining (8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TP0MhpuGTGD_",
    "outputId": "c5807fea-8741-4e80-cfa8-8971c49202be"
   },
   "outputs": [],
   "source": [
    "print('Specific_classes to be examined:\\n')\n",
    "classes= {0:'N',1:'L',2:'R',3:'V',4:'/',5:'A',6:'f',7:'F'}\n",
    "print(classes,'\\n')\n",
    "for k,v in classes.items():\n",
    "    print('Class {} condition: {}'.format(k+1,hb.classes_further[v]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3A1jC4ALTGEB",
    "outputId": "749a27ac-f699-493c-f3a4-97bf7ee5b704",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "classes= {0:'N',1:'L',2:'R',3:'V',4:'/',5:'A',6:'f',7:'F'}\n",
    "patient_dic=hb.distribution_bar(patients=hb.all_patients(),classes=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "USEEH4qyTGED"
   },
   "source": [
    "### Lets' Reduce the Number of Classes. Not all patients are represented (Creating a connecting Dictionary)\n",
    "\n",
    "all_classes --> {0:'N',1:'S',2:'V',3:'F',4:'Q'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tEFcNmhcTGEE",
    "outputId": "8fdb8a77-48c4-4389-dba0-1cf817fdb3cc"
   },
   "outputs": [],
   "source": [
    "classes= {0:'N',1:'S',2:'V',3:'F',4:'Q'}\n",
    "classes_reducer={'N':['N','L','R','e','j'],\n",
    "                 'S':['S','A','a','J'],'V':['V','E'],'F':['F'],'Q':['/','Q','f']}\n",
    "for c, subclass in classes_reducer.items():\n",
    "    print('For class:',c, '({})'.format(hb.classes_further[c]))\n",
    "    for i in subclass:\n",
    "        print(\"  ({})\".format(i), hb.classes_further[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9ZQjj4pBTGEH",
    "outputId": "58e46a07-67b4-4296-c7f8-3343460361b3"
   },
   "outputs": [],
   "source": [
    "patient_dic=hb.distribution_bar(patients=hb.all_patients(),classes=classes,\n",
    "                                classes_reducer=classes_reducer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DiWXnGvrTGEJ"
   },
   "source": [
    "We notice a better distribution with joined classes. If we remove Normal conditions we can see this a little more clearly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u30kn1MuTGEJ",
    "outputId": "0124cd10-2d6e-4890-a173-1c94b4e3ea6c"
   },
   "outputs": [],
   "source": [
    "classes_wo_N= {0:'S',1:'V',2:'F',3:'Q'}\n",
    "patient_dic=hb.distribution_bar(patients=hb.all_patients(),classes=classes_wo_N,\n",
    "                                classes_reducer=classes_reducer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2s04KnmgTGEM"
   },
   "source": [
    "### Examing sample heart rate data \n",
    "#### Run Cell As Many Times as you like to see how different patients look like (Peaks in Orange)\n",
    "`hb.all_patients` : list of all patients 100 ~ 234 (48 total)<br/>\n",
    "`hb.get_patient_data(patient,norm=True, sample_plot=False)`: function to plot<br/>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Assumes that all folder called mit_data is next folder \n",
    "    in current directory. \n",
    "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Can change this function internally or write your own personalized one. <br/>\n",
    "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Input: \n",
    "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`patient`: Patient Number [Str or Int]\n",
    "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`norm`: (optional) =True --> Normalize Data \n",
    "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`sample_plot`: (optional) Show Patient ECG Signal [True or False]\n",
    "<br/><br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Output: (If sample_plot == True: only output is a sample figure plot)\n",
    "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Normalized Signal Data, Ecg Notes \n",
    "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`Ecg_Notes`: Labeled Sample Peaks and Heart Conditions \n",
    "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`Ecg_Data`: np.array of signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YlY17hQYTGEN",
    "outputId": "9b2b0eb0-97d1-4cca-ce61-289db16adaf8"
   },
   "outputs": [],
   "source": [
    "patient=random.choice(hb.all_patients())\n",
    "hb.get_patient_data(patient,norm=False,sample_plot=True)\n",
    "plt.ylabel('Un-normalized Voltage')\n",
    "plt.xlabel('Sample #')\n",
    "plt.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yto9YEw7TGEQ",
    "outputId": "668bb444-5ea4-4604-cb00-1cc1774f56ae",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "signal,ecg_notes=hb.get_patient_data(patient=101,norm=False,sample_plot=False)\n",
    "from IPython.display import display_html\n",
    "def display_side_by_side(*args):\n",
    "    html_str=''\n",
    "    for df in args:\n",
    "        html_str+=df.to_html()\n",
    "    display_html(html_str.replace('table','table style=\"display:inline\"'),raw=True)\n",
    "\n",
    "print(\"Labeled Heart Beats  | Signal Data\")\n",
    "display_side_by_side(ecg_notes.head(),signal.to_frame().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wXFDawPGTGET"
   },
   "source": [
    "### Examining the Best Techniques to Normalizing data \n",
    "`hb.moving_average`: Numpy based moving average function.\n",
    "    Input: signal and window size |\n",
    "    Output: averaged signal\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C-9N7bCOTGET",
    "outputId": "92bd9289-ba74-4317-d7d7-82d4667be305"
   },
   "outputs": [],
   "source": [
    "patient=random.choice(hb.all_patients())\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def compare(patient,length,window=10):\n",
    "    print(\"PATIENT: {}\\n\".format(patient))\n",
    "    sig,notes= hb.get_patient_data(patient,norm=False)\n",
    "    m=hb.moving_average(sig,10)\n",
    "    n = np.random.choice(10*length)\n",
    "    plt.figure(figsize=(20,10))\n",
    "    \n",
    "    plt.subplot(221)\n",
    "    plt.plot(sig[0:length])\n",
    "    plt.plot(m[0:length],label='Moving avg')\n",
    "    plt.legend()\n",
    "    plt.title('Original Signal')\n",
    "    \n",
    "    \n",
    "    print(\"Norm 1: Creates a mean of 0, and a std of 1\")\n",
    "    plt.subplot(222)\n",
    "    plt.plot(normalizer.z_norm_b(sig)[0:length])\n",
    "    m=hb.moving_average(normalizer.z_norm_b(sig),window)\n",
    "    plt.plot(m[0:length],label='Moving avg')\n",
    "    plt.legend()\n",
    "    plt.title('Norm 1')\n",
    "    \n",
    "    print(\"Norm 2: Creates a mean of 0, and y range from [-1 1]\")\n",
    "    plt.subplot(223)\n",
    "    plt.plot(normalizer.z_norm2(sig)[0:length])\n",
    "    m=hb.moving_average(normalizer.z_norm2(sig),window)\n",
    "    plt.plot(m[0:length],label='Moving avg')\n",
    "    plt.legend()\n",
    "    plt.title('Norm 2')\n",
    "\n",
    "    \n",
    "    print(\"Norm 3: Creates a y range from [0 1]\")\n",
    "    plt.subplot(224)\n",
    "    plt.plot(normalizer.z_norm(sig)[0:length])\n",
    "    m=hb.moving_average(normalizer.z_norm(sig),window)\n",
    "    plt.plot(m[0:length],label='Moving avg')\n",
    "    plt.legend()\n",
    "    plt.title('Norm 3')\n",
    "    plt.show()\n",
    "    \n",
    "compare(patient=patient,length=300)\n",
    "print('Selected Norm = 3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IupqGfaaTGEW",
    "outputId": "94b6a4fc-6574-4d0b-e1ab-c1fc3f28ed33"
   },
   "outputs": [],
   "source": [
    "hb.get_patient_data(patient=231,norm=True,sample_plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X7S8AFo7TGEY"
   },
   "source": [
    "### NORM 3 selected and used to normalize data \n",
    "`hb.get_patient_data()` with norm flag = True\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1zy2LKBRTGEZ",
    "outputId": "462bdd78-1424-479d-8550-31fa2ed8dfd1"
   },
   "outputs": [],
   "source": [
    "patient=random.choice(hb.all_patients())\n",
    "\n",
    "hb.get_patient_data(patient,norm=True,sample_plot=True)\n",
    "plt.ylabel('Normalized Voltage')\n",
    "plt.xlabel('Sample #')\n",
    "plt.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BnhVE9qcTGEb"
   },
   "source": [
    "### Examing Relevance of Frequency in the Model\n",
    "#### Run Cell As Many Times as you like to see how different patients look like (Patient Num as Title)\n",
    "\n",
    "`filt` is a class of various filtration functions \n",
    "\n",
    "`filt.low_pass_filter_plot(self,patient,cutoff,fs=360,order=5)` function to show butter worth low pass filter impact<br/>\n",
    "\n",
    "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Input: \n",
    "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`patient`: Patient Number [Str or Int]\n",
    "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`cutoff`: cutoff frequency\n",
    "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`fs`: sample frequency (samples/sec)\n",
    "<br/><br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Output: (Subplots) \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;FFT (FAST \n",
    "FOURIER TRANSFORM)\n",
    "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Filter Frequency Response\n",
    "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Example of Filtered signal compared to orignal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bL9hguh_TGEc",
    "outputId": "704a3621-c5d4-48f4-f43c-2a1e53835ed4"
   },
   "outputs": [],
   "source": [
    "from normalizer import filt  #class of various filteration functions \n",
    "patient= random.choice(hb.all_patients())\n",
    "filt().low_pass_filter_plot(patient=patient,cutoff=20,fs=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ga4HgNvFTGEe"
   },
   "source": [
    "### Heart Beat Isolation Algorithm \n",
    "##### Isolated a long ECG Singal into Peaks: Function can be adapter with `biosppy.christov_segementer`:: https://biosppy.readthedocs.io/en/stable/biosppy.signals.html\n",
    "\n",
    "`hb.isolate_patient_data(patients,classes,classes_further,min_HR= 40,max_HR= 140,fs=360,verbose=False,plot_figs=True)`: isolation algorithim\n",
    "\n",
    "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Input: \n",
    "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`patients`: LIST of Patient Numbers [Str or Int]\n",
    "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`classes`: \n",
    "classes to be examined {dic}\n",
    "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`classes_further`: (hb.classes_further) expansion of previous classes with names {dic}\n",
    "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`classes_reducer`: (None) optional argument that can be used to link one class to another\n",
    "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`min_HR`: (optional) minimum HR to consider (longer HR Sample Rate)\n",
    "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`max_HR`: (optional) max HR to consider (longer HR Sample Rate)\n",
    "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`fs`: (optional) sampling frequency --> 360 for this database\n",
    "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`verbose`: (optional) prints out some information per patient if true [boolean]<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`plot_figs`: \n",
    "plot_figs:: (optional) prints out HR and Heat Beat distributions \n",
    "\n",
    "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Output: \n",
    "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;X, y, isolated Beats  \n",
    "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`X`: np arrays of each heart beat signal (normed + zero_padded)\n",
    "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`y`: np arrays of [patient, HR, and HeartBeat Class]\n",
    "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`Isolated Beat`: list of lists (unpadded) of signal and associated labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uyLqdXzjTGEe",
    "outputId": "c73de60a-7d45-405b-e378-33640e26ae22"
   },
   "outputs": [],
   "source": [
    "#isolation algorithim heart beat data \n",
    "X,y,isolated_beat= hb.isolate_patient_data(patients=hb.all_patients(),classes=classes,\n",
    "                    classes_further=hb.classes_further, classes_reducer=classes_reducer, \n",
    "                     min_HR= 40,max_HR= 140,fs=360,verbose=False,plot_figs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yLHqSuSqTGEg",
    "outputId": "3225dede-000d-48ed-ab69-d6c4fd81d759"
   },
   "outputs": [],
   "source": [
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "print(\"sample y vals:: [patient#, HR, Condition Class]:\",y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GWfM4Pt6TGEi"
   },
   "source": [
    "### Looking into Isolated Heart Beats \n",
    "\n",
    "Grouping these classes is actually proves to be realistic\n",
    "\n",
    "\n",
    "`hb.show_sample_plots(X,y,classes,classes_further,num_sigs=5,fs=360,plot_xlim=1)`: isolation algorithim\n",
    "\n",
    "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Input: \n",
    "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`X`: np arrays of each heart beat signal (normed + zero_padded)\n",
    "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`y`: np arrays of [patient, HR, and HeartBeat Class]\n",
    "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`Isolated Beat`: list of lists (unpadded) of signal and associated labelsLIST of Patient Numbers [Str or Int]\n",
    "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`classes`: \n",
    "classes to be examined {dic}\n",
    "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`classes_further`: (hb.classes_further) expansion of previous classes with names {dic}\n",
    "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`num_sigs`: number of signals (per class) to over lay\n",
    "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`fs`: (optional) sampling frequency --> 360 for this database\n",
    "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`plot_xlim`: number of seconds to show (1s) on x axis \n",
    "\n",
    "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Output: \n",
    "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Heart Beat Subplots per Class  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I7Jacw7OTGEi",
    "outputId": "0535e4f2-6e04-4a81-ac52-6564b5a4d7cb"
   },
   "outputs": [],
   "source": [
    "hb.show_sample_plots(X=X,y=y,classes=classes,classes_further=hb.classes_further,plot_xlim=1,dims=[2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hXYF1e6-TGEk"
   },
   "source": [
    "### Resampling 360 Hz (How many data points do we need with out losing too much information per signal?)\n",
    "\n",
    "##### (360Hz/2 ~ 180-190 Hz) \n",
    "\n",
    "\n",
    "`hb.resample_vals(X,samp_len)`: Resampling Algorithim \n",
    "\n",
    "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Input: \n",
    "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`X`: np arrays of each heart beat signal (normed + zero_padded)\n",
    "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`sample_len`: New Length to Resample. reample_len must be < len(X[n]) to down sample \n",
    "\n",
    "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Output: \n",
    "<br/>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; X resampled  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VR7VFVDmTGEl",
    "outputId": "f0d1703e-3b98-4675-9e4d-3120cb4c7a43"
   },
   "outputs": [],
   "source": [
    "# Cut Roughly in Half \n",
    "print('Resampling...\\n')\n",
    "X_resamp=hb.resample_vals(X,samp_len=187)\n",
    "print(\"X_resamp shape:\", X_resamp.shape)\n",
    "print(\"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hJYKXNf6TGEo"
   },
   "source": [
    "### How Resampling (looking at about 1/2 second) looks like. \n",
    "##### Singals are all preseved with out the model getting to occupied with to much data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iTxAwk2FTGEo",
    "outputId": "26f95e90-62b6-473a-9e05-576112aa4873"
   },
   "outputs": [],
   "source": [
    "hb.show_sample_plots(X=X_resamp,y=y,classes=classes,classes_further=hb.classes_further,\n",
    "                     plot_xlim=.5,dims=[2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uLNVksq0TGEq"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(X_resamp).to_csv(\"X_resamped.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RHV_GS32TGEs"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(y).to_csv(\"y.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eciRUiYBTUVr"
   },
   "source": [
    "# Deep Learning \n",
    "\n",
    "#### 1. Dividing and Preparing Data For Training \n",
    "#### 2. Design Model In Pytorch (CUDA)\n",
    "#### 3. Develop Training and Evaluation Metrics \n",
    "#### 4. Results\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p3Tvh9GnTGEv"
   },
   "outputs": [],
   "source": [
    "classes={0: 'N', 1: 'S', 2: 'V', 3: 'F', 4: 'Q'}\n",
    "print('X (resampled):',X_resamp.shape,'y:',y.shape,'Classes:',classes)\n",
    "\n",
    "def get_key(val,my_dict): \n",
    "    \"\"\"\n",
    "    Simple Function to Get Key \n",
    "    in Dictionary from val. \n",
    "    \n",
    "    Input: Key, Dictionary \n",
    "    Output: Val\n",
    "    \n",
    "    \"\"\"\n",
    "    for key, value in my_dict.items(): \n",
    "         if val == value: \n",
    "            return key \n",
    "    return \"key doesn't exist\"\n",
    "\n",
    "def one_hot(c,classes=classes):\n",
    "    \"\"\"\n",
    "    Simple one hot encoding for the \n",
    "    types of arrthymia conditions. \n",
    "    \n",
    "    class --> encode class\n",
    "    'N' --> [1, 0, 0, 0, 0, 0, 0, 0]\n",
    "    \n",
    "    c:: current class of the object\n",
    "    classes:: classes dictionary \n",
    "        \n",
    "    \"\"\"\n",
    "    enc=np.zeros(len(classes),dtype=int).tolist()\n",
    "    enc[get_key(c,classes)]= 1\n",
    "    return enc\n",
    "  \n",
    "all_patients=['100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '111', '112', '113', '114', '115', '116', '117', '118', '119', '121',\n",
    "              '122', '123', '124', '200', '201', '202', '203', '205', '207', '208', '209', '210', '212', '213', '214', '215', '217', '219', '220', '221', \n",
    "              '222', '223', '228', '230', '231', '232', '233', '234']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S_54Q6upV-Cl"
   },
   "source": [
    "### Dividing and Prepping Data for DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "l21a0aoVTGEw",
    "outputId": "0c492706-1f74-4a01-a35c-7d8659449d4f"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "def get_train_test(X,y,train_size,classes=classes,patients=all_patients):\n",
    "  \"\"\"\n",
    "  Get train and test function for spliting ensuring testing has all classes\n",
    "  preseting for testing/eval to see how well all classes are performing. \n",
    "  \"\"\"\n",
    "\n",
    "  sub_c={}\n",
    "    \n",
    " \n",
    "  for c in classes:\n",
    "      C = np.argwhere(y[:,2] == list(classes.values())[c]).flatten()\n",
    "\n",
    "      sub_c[c]=np.random.choice(C,int((C.shape[0]- C.shape[0]*train_size)))\n",
    "\n",
    "  X_test = np.vstack([X[sub_c[0]], X[sub_c[1]], X[sub_c[2]], X[sub_c[3]], X[sub_c[4]]])\n",
    "  y_test = np.vstack([y[sub_c[0]], y[sub_c[1]], y[sub_c[2]], y[sub_c[3]], y[sub_c[4]]])\n",
    "\n",
    "  deletions=[]\n",
    "  for i in range(len(sub_c)):\n",
    "    deletions.extend(sub_c[i].tolist())\n",
    "  \n",
    "  X_train = np.delete(X, deletions, axis=0)\n",
    "  y_train = np.delete(y, deletions, axis=0)\n",
    "\n",
    "  X_train, y_train = shuffle(X_train, y_train, random_state=0)\n",
    "  X_test, y_test = shuffle(X_test, y_test, random_state=0)\n",
    "  y_train= np.array([get_key(y_i,classes) for y_i in y_train[:,2]])\n",
    "  y_test= np.array([get_key(y_i,classes) for y_i in y_test[:,2]])\n",
    "  return X_train,y_train,X_test,y_test\n",
    "\n",
    "  \n",
    "\n",
    "X_train,y_train,X_test,y_test=get_train_test(X=X_resamp,y=y,train_size=.80)\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_test:\", y_test.shape)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WpqwOS4kTGEz"
   },
   "source": [
    "#### Get Train, Validation, and Test Loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "id": "5lJQo3vFTGE0",
    "outputId": "e9af813a-07d7-472a-8833-8eb3d25bea01"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "import torch\n",
    "from torch import optim \n",
    "import random\n",
    "import torch.utils.data\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import random\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def imbalanced_loader(X_train,y_train,X_test,y_test,valid_size=.05,batch_size=512): # Split train into train + validation \n",
    "    \"\"\"\n",
    "    Get trainloader, validloader, and testloader for model training. This \n",
    "    creates equal training batches but naturally balanced validation and testing \n",
    "    sets. Note the testing set was previously augmented to get better per class metrics \n",
    "    \n",
    "    Outputs: dataloader + testloader, where dataloader =  {\"train\": trainloader, \"val\": validloader}\n",
    "\n",
    "    \"\"\"\n",
    "    warnings.filterwarnings(\"ignore\") #torch bug\n",
    "    print ('Getting Data... {}% Validation Set\\n'.format(int(np.around(valid_size*100))))\n",
    "    \n",
    "    num_train = len(X_train)\n",
    "    indices = list(range(num_train))\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "    \n",
    "    print(\"Batch Size:\",batch_size)\n",
    "\n",
    "    print('\\nTrain Len=',len(train_idx),', Validation Len=',len(valid_idx), 'Test Len=',len(y_test))\n",
    "                                                                                        \n",
    "  \n",
    "    class_sample_count = np.array([len(np.where(y_train[train_idx]==t)[0]) for t in np.unique(y_train[train_idx])])\n",
    "    weight = 1. / class_sample_count\n",
    "    samples_weight = np.array([weight[t] for t in y_train[train_idx]])\n",
    "    samples_weight = torch.from_numpy(samples_weight)\n",
    "    train_sampler = WeightedRandomSampler(torch.tensor(samples_weight,dtype=torch.double), len(samples_weight))\n",
    "    trainDataset = torch.utils.data.TensorDataset(torch.FloatTensor(X_train[train_idx]), torch.LongTensor(y_train[train_idx].astype(int)))\n",
    "    train_sampler= torch.utils.data.BatchSampler(sampler=train_sampler, batch_size=batch_size, drop_last=True)\n",
    "    trainloader = torch.utils.data.DataLoader(dataset = trainDataset, batch_size=batch_size, num_workers=1, sampler= train_sampler)\n",
    "  \n",
    "    \n",
    "    valDataset = torch.utils.data.TensorDataset(torch.FloatTensor(X_train[valid_idx]), torch.LongTensor(y_train[valid_idx].astype(int)))\n",
    "    sampler = torch.utils.data.RandomSampler(valDataset)\n",
    "    sampler= torch.utils.data.BatchSampler(sampler, batch_size, drop_last=True)\n",
    "    validloader = torch.utils.data.DataLoader(dataset = valDataset, batch_size=batch_size, num_workers=1,sampler=sampler)\n",
    "\n",
    "\n",
    "    testset=[]\n",
    "    for i,x in enumerate(X_test):\n",
    "        testset.append((torch.from_numpy(x),torch.tensor([y_test[i]])))\n",
    "    \n",
    "    #testloader = torch.utils.data.DataLoader(dataset = testDataset, batch_size=batch_size, shuffle=False, num_workers=1) \n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                           shuffle=False, num_workers=1)\n",
    "\n",
    "    print(\"\")\n",
    "    dataloader = {\"train\": trainloader, \"val\": validloader}\n",
    "    print('Train Size Batched=',int(len(dataloader['train'].dataset)/batch_size),', Validation Size Batched=',int(len(dataloader['val'].dataset)/batch_size),', Test Size Batched=',len(testloader))\n",
    "    \n",
    "    \n",
    "    warnings.resetwarnings()\n",
    "    return dataloader,testloader\n",
    "  \n",
    "batch_size=512\n",
    "dataloader,testloader = imbalanced_loader(X_train=X_train,y_train=y_train,X_test=X_test,y_test=y_test, valid_size=.05,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J5LKXaRnTGE1"
   },
   "source": [
    "#### Lets see how classes are balanced for training \n",
    "(Testing and Validation sets have natural balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2788
    },
    "colab_type": "code",
    "id": "scF3-58pTGE1",
    "outputId": "aa56889f-4c96-4fb3-b20f-93f0f25e6864"
   },
   "outputs": [],
   "source": [
    "for hb,labels in dataloader['train']:\n",
    "    for hb_index,label in enumerate(labels):\n",
    "        print(hb_index,hb[hb_index].size(),label.cpu().numpy().shape,Counter(label.cpu().numpy().flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b_-qk7isTGE2"
   },
   "source": [
    "### Model Architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "10GLau7CTGE3",
    "outputId": "79d3ef9c-1b27-46ff-abdc-bb7f4a323c9f"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "print(\"\"\"\\nA 1D CNN is very effective when you expect to derive interesting features from shorter \n",
    "(fixed-length) segments of the overall data set and where the location of the feature \n",
    "within the segment is not of high relevance.\\n\"\"\")\n",
    "\n",
    "class Anomaly_Classifier(nn.Module):\n",
    "    def __init__(self, input_size,num_classes):\n",
    "        super(Anomaly_Classifier, self).__init__()\n",
    "    \n",
    "        self.conv= nn.Conv1d(in_channels=input_size, out_channels=32, kernel_size=5,stride=1)\n",
    "        \n",
    "        self.conv_pad = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=5,stride=1,padding=2)\n",
    "        self.drop_50 = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=5,stride=2) \n",
    "\n",
    "        self.dense1 = nn.Linear(32 * 8, 32) \n",
    "        self.dense2 = nn.Linear(32, 32) \n",
    "        \n",
    "        self.dense_final = nn.Linear(32, num_classes)\n",
    "        self.softmax= nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual= self.conv(x)\n",
    "      \n",
    "        #block1 \n",
    "        x = F.relu(self.conv_pad(residual))\n",
    "        x = self.conv_pad(x)\n",
    "        x+= residual \n",
    "        x = F.relu(x)\n",
    "        residual = self.maxpool(x) #[512 32 90]\n",
    "       \n",
    "        #block2\n",
    "        x=F.relu(self.conv_pad(residual))\n",
    "        x=self.conv_pad(x)\n",
    "        x+=residual\n",
    "        x= F.relu(x)\n",
    "        residual = self.maxpool(x) #[512 32 43]\n",
    "        \n",
    "        \n",
    "        #block3\n",
    "        x=F.relu(self.conv_pad(residual))\n",
    "        x=self.conv_pad(x)\n",
    "        x+=residual\n",
    "        x= F.relu(x)\n",
    "        residual = self.maxpool(x) #[512 32 20]\n",
    "        \n",
    "        \n",
    "        #block4\n",
    "        x=F.relu(self.conv_pad(residual))\n",
    "        x=self.conv_pad(x)\n",
    "        x+=residual\n",
    "        x= F.relu(x)\n",
    "        x= self.maxpool(x) #[512 32 8]\n",
    "        \n",
    "        #MLP\n",
    "        x = x.view(-1, 32 * 8) #Reshape (current_dim, 32*2)\n",
    "        x = F.relu(self.dense1(x))\n",
    "        #x = self.drop_60(x)\n",
    "        x= self.dense2(x)\n",
    "        x = self.softmax(self.dense_final(x))\n",
    "        return x\n",
    "      \n",
    "print('Model Architecture Init\\n')\n",
    "\n",
    "print(\"OPTIMIZER = optim.Adam(anom_classifier.parameters(),lr = 0.001) \\n \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xUZ1VfNtTGE4"
   },
   "source": [
    "### Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0Icaxy2DTGE5",
    "outputId": "9b10a0c5-7a3b-4e7c-84f1-dedb8f3a9718"
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import torch.optim as optim\n",
    "import time \n",
    "import sklearn \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "\n",
    "def reset_weights(model):\n",
    "  \"\"\"\n",
    "  model.apply(reset_weights) will reset all the model parameters.\n",
    "  This way the model is not overwhelmed \n",
    "  \n",
    "  \"\"\"\n",
    "  if isinstance(model, nn.Conv1d) or isinstance(model, nn.Linear):\n",
    "      model.reset_parameters()\n",
    "      \n",
    "def calc_accuracy(output,Y):\n",
    "  \n",
    "    # get acc_scores during training \n",
    "    max_vals, max_indices = torch.max(output,1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc\n",
    "  \n",
    "def train_model(data_loader, model, criterion,optimizer, n_epochs=100,print_every=10,verbose=True,plot_results=True,validation=True):\n",
    "  \n",
    "  \"\"\"\n",
    "  Model Training Function.\n",
    "  Input:\n",
    "    \n",
    "    Dataloader: {'train':trainloader,'val':validloader} --> If no validation is used set Validation = False & dataloader= {'train':trainloader}\n",
    "    model: model.cuda() if gpu will be used, else cpu\n",
    "    print_every: print every n epochs \n",
    "    verbose: print out results per epoch \n",
    "    plot_results: plot the train and valid loss \n",
    "    validation: is validation set in dataloader\n",
    "  \n",
    "  Output:\n",
    "  \n",
    "    trained classifier \n",
    "  \n",
    "  \"\"\"\n",
    "\n",
    "  losses=[]\n",
    "  start= time.time()\n",
    "  print('Training for {} epochs...\\n'.format(n_epochs))\n",
    "  for epoch in range(n_epochs):\n",
    "      if verbose == True and epoch % print_every== 0:\n",
    "        print('\\n\\nEpoch {}/{}:'.format(epoch+1, n_epochs))\n",
    "        \n",
    "      if validation == True: \n",
    "        evaluation=['train', 'val']\n",
    "      else:\n",
    "        \n",
    "        evaluation=['train']\n",
    "        \n",
    "      # Each epoch has a training and validation phase\n",
    "      for phase in evaluation:\n",
    "          if phase == 'train': \n",
    "              model.train(True)  # Set model to training mode\n",
    "          else:\n",
    "              model.train(False)  # Set model to evaluate mode\n",
    "\n",
    "          running_loss = 0.0\n",
    "\n",
    "          # Iterate over data.\n",
    "          for hb,labels in data_loader[phase]:\n",
    "            for hb_index,label in enumerate(labels):\n",
    "#                 print(hb[hb_index].size(),label.cpu().numpy().shape,Counter(label.cpu().numpy().flatten()))\n",
    "                HB, label = hb[hb_index].unsqueeze(1).cuda(), label.cuda()\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                outputs = model(HB)\n",
    "                acc= calc_accuracy(outputs,label)\n",
    "                loss = criterion(outputs, label)#loss function \n",
    "                # zero the parameter (weight) gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    # update the weights\n",
    "                    optimizer.step()\n",
    "\n",
    "                # print loss statistics\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            losses.append(running_loss) \n",
    "\n",
    "          if verbose == True and epoch % print_every== 0: \n",
    "            print('{} loss: {:.4f} | acc: {:.4f}|'.format(phase, running_loss,acc), end=' ')\n",
    "  if verbose == True:        \n",
    "    print('\\nFinished Training  | Time:{}'.format(time.time()-start))\n",
    "  if plot_results == True:\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot(losses[0::2],label='train_loss')\n",
    "    if validation == True:\n",
    "      plt.plot(losses[1::2],label='validation_loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.draw()\n",
    "  \n",
    "  return model \n",
    "\n",
    "\n",
    "\n",
    "def evaluate(testloader, trained_model,verbose= True):\n",
    "  \"\"\"\n",
    "  Evaluation Metric Platfrom. Feed in the trained model \n",
    "  and test loader data. \n",
    "  \n",
    "  Returns classification metric along with \n",
    "  predictions,truths\n",
    "  \n",
    "  \"\"\"\n",
    "  \n",
    "  truth=[]\n",
    "  preds=[]\n",
    "  for hb,label in testloader:\n",
    "      HB, label = hb.float().unsqueeze(1).cuda(), label.cuda()\n",
    "      outputs = trained_model(HB)\n",
    "      _, predicted = torch.max(outputs, 1)\n",
    "      preds.append(predicted.cpu().numpy().tolist())\n",
    "      truth.append(label.cpu().numpy().tolist())\n",
    "  \n",
    "  preds_flat = [item for sublist in preds for item in sublist]\n",
    "  truth_flat = [item for sublist in truth for item in sublist] \n",
    " \n",
    "\n",
    "  if verbose == True:\n",
    "    print('\\nEvaluating....')\n",
    "    print(\"TEST ACC:\",accuracy_score(truth_flat,preds_flat))\n",
    "    print(classification_report(truth_flat,preds_flat))\n",
    "  \n",
    "  return preds_flat,truth_flat\n",
    "\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "    \n",
    "    \n",
    "def get_kernel_size(n_h,k_h,n_w,k_w,p_h=0,s_h=1,p_w=0,s_w=1):\n",
    "    \"\"\"\n",
    "    Kernel Measuring Function \n",
    "    \"\"\"\n",
    "    return [int((n_h-k_h+p_h+s_h)/s_h),int((n_w-k_w+p_w+s_w)/s_w)]    \n",
    "    \n",
    "    \n",
    "def variation(n_epochs,num_iters=5):\n",
    "  p=[]\n",
    "  t=[]\n",
    "  accuracy_scores=[]\n",
    "  for i in range(num_iters):\n",
    "    print('\\nModel {}/{}...\\n'.format(i+1,num_iters))\n",
    "    Anomaly_Classifier(input_size=1,num_classes= 5).cuda().apply(reset_weights)\n",
    "    print('Weights Reset')\n",
    "    anom_classifier= Anomaly_Classifier(input_size=1,num_classes= 8).cuda()\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = optim.Adam(anom_classifier.parameters(),lr = 0.001) \n",
    "    trained_classifier= train_model(data_loader=dataloader, model=anom_classifier,\n",
    "                                    criterion = criterion,optimizer = optimizer ,\n",
    "                                    n_epochs=n_epochs,print_every=1,verbose=False,plot_results=False, \n",
    "                                    validation=True)\n",
    "    \n",
    "    preds,truth = evaluate(testloader=testloader, trained_model = trained_classifier,verbose=False)\n",
    "    t.append(truth)\n",
    "    p.append(preds)\n",
    "    print(accuracy_score(truth,preds))\n",
    "    accuracy_scores.append(accuracy_score(truth,preds))\n",
    "  return p,t,accuracy_scores\n",
    "print('Functions Ready')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OG3wvB2Wg2ab"
   },
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1476
    },
    "colab_type": "code",
    "id": "SLNf3sGRTGE7",
    "outputId": "7c34e946-f2e6-403e-8519-201d3b570f3a"
   },
   "outputs": [],
   "source": [
    "Anomaly_Classifier(input_size=1,num_classes= 5).cuda().apply(reset_weights)\n",
    "print('Weights Reset')\n",
    "anom_classifier= Anomaly_Classifier(input_size=1,num_classes= 5).cuda()\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(anom_classifier.parameters(),lr = 0.001) \n",
    "trained_classifier2 = train_model(data_loader=dataloader, model=anom_classifier, criterion = criterion,optimizer = optimizer ,\n",
    "                                            n_epochs=100,print_every=10,verbose=True,plot_results=True, validation=True)\n",
    "preds,truth = evaluate(testloader=testloader, trained_model = trained_classifier2,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 746
    },
    "colab_type": "code",
    "id": "39SEF4aDTGE8",
    "outputId": "be4e5faa-e0b6-4fa1-bfbc-852994e00a1f"
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true=truth, y_pred=preds)\n",
    "plot_confusion_matrix(cm=cm,normalize=True,classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1006
    },
    "colab_type": "code",
    "id": "uZZqGELIm_fI",
    "outputId": "d4db57ae-821e-41cd-92f2-d669dfcb692d"
   },
   "outputs": [],
   "source": [
    "s=time.time()\n",
    "acc=variation(n_epochs=100,num_iters=5)\n",
    "time.time()-s\n",
    "import seaborn as sns \n",
    "print(acc)\n",
    "sns.boxplot(acc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "rZqrtKCuTGEG",
    "J5LKXaRnTGE1"
   ],
   "name": "Eddy_Mina_DSFINAL.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
